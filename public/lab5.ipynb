{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183e38fc-1d44-4219-b7bd-a49784d13f9a",
   "metadata": {},
   "source": [
    "# Lab 5: Regression and Regularization\n",
    "\n",
    "In today’s lab, we'll analyze a dataset from a **servo control system**, which models how different mechanical and electrical components influence the system's response time. A **servo** is a type of motor used in engineering and robotics to precisely control movement. The dataset consists of historical simulation data, where various factors—such as motor type, screw type, and gain settings—impact the time it takes for the system to respond to a step input.\n",
    "\n",
    "Using this data, we will build a regression model to predict the rise time of the servo mechanism based on its configuration. Understanding how these factors affect response time is crucial in designing efficient and reliable control systems, making this an important problem in engineering and automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b0673-5b13-4a1a-acd8-d5028af0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL - imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34364970-fad0-4118-8b80-d9cb2fc0e654",
   "metadata": {},
   "source": [
    "### Task 1: Loading in the data\n",
    "\n",
    "We're going to be using a dataset from the **UCI Machine Learning Repository.** Go to the dataset website [here](https://archive.ics.uci.edu/dataset/87/servo) and follow the steps to **import the dataset in Python**.\n",
    "- URL also linked for convenience: https://archive.ics.uci.edu/dataset/87/servo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c406be-f4bc-4f7a-befd-b7b246b6f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a19aa-6667-4206-9035-35dff76c0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d550c94-90d0-49aa-9564-47342f5d4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL - Sanity Check\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24670039-ef7f-4b07-b7f5-223e32856736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL - Sanity Check\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d4e56-5046-44d5-88b0-221721766823",
   "metadata": {},
   "source": [
    "### Task 2: Understanding our Features\n",
    "\n",
    "In our dataset `X`, we have 4 features:\n",
    "\n",
    "- **`motor`**: A categorical variable representing the type of motor used in the servo system. Possible values are **A, B, C, D, and E**.\n",
    "- **`screw`**: A categorical variable representing the type of screw in the system, with possible values **A, B, C, D, and E**.\n",
    "- **`pgain`**: An integer representing the proportional gain setting of the servo, which influences how aggressively the system corrects for errors. Possible values range from **3 to 6**.\n",
    "- **`vgain`**: An integer representing the velocity gain setting, which affects how the system responds to velocity changes. Possible values range from **1 to 5**.\n",
    "\n",
    "Our goal is to use these features to predict the **rise time** of the servo (the `class` column in our target variable `y`), a continuous numerical value that indicates how quickly the system responds to a step input.\n",
    "\n",
    "Visualize the distribution of each variable using a **bar plot**: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html\n",
    "\n",
    "**Hint**: You will need to use `pandas.Series.value_counts()`\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3280b2-6309-4567-bbd6-65cafda1ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER GOES HERE - Visualize the distribution of motor types\n",
    "...\n",
    "plt.xlabel(\"Motor Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Motor Types for Servo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38163e-f6fd-4e33-b161-d249209793a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER GOES HERE - Visualize the distribution of screw types\n",
    "...\n",
    "plt.xlabel(\"Screw Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Screw Types for Servo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18551e-bbc7-43ec-84cf-487dc9f5b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER GOES HERE - Visualize the distribution of pgain settings\n",
    "...\n",
    "plt.xlabel(\"Proportional Gain Setting\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Proportional Gain Setting for Servo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e3e02-769c-44db-ab80-875040bd9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER GOES HERE - Visualize the distribution of vgain settings\n",
    "...\n",
    "plt.xlabel(\"Velocity Gain Setting\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Velocity Gain Setting for Servo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf7656-4ca0-4efc-9d22-50c5b9902c74",
   "metadata": {},
   "source": [
    "### Task 3: Visualizing Our Target\n",
    "\n",
    "The goal of our regression model is going to be to predict rise time `y`. Create a **scatterplot** (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) of:\n",
    "- `pgain` vs `y` and print the correlation coefficient (https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)\n",
    "- `vgain` vs `y` and print the correlation coefficient (https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86326366-6db4-4a18-8c0a-d0539c1563ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER GOES HERE - Scatterplot of pgain vs. class\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3119546-8249-4dbc-b915-03b5f6f081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER GOES HERE - Scatterplot of vgain vs. class\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf85906-cd64-49be-bf5c-67cb09a56325",
   "metadata": {},
   "source": [
    "You might notice that the scatterplots look a little weird. That's because even though the `pgain` and `vgain` variables are integers (numeric), they are actually *discrete* values, and there are only 4 possible `pgain` values and 5 possible `vgain` values.\n",
    "\n",
    "A better way to visualize this data might be doing side-by-side box plots so that we can visualize the distribution of servo rise time within each discrete bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f81fe1-82fa-4dfa-90cd-aed81411afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "df = X.copy()\n",
    "df[\"class\"] = y[\"class\"]\n",
    "\n",
    "# Create subplots for pgain and vgain\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Box plot for pgain\n",
    "df.boxplot(column='class', by='pgain', ax=axs[0])\n",
    "axs[0].set_title('Rise Time Distribution by Pgain')\n",
    "axs[0].set_xlabel('Pgain')\n",
    "axs[0].set_ylabel('Rise Time')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Box plot for vgain\n",
    "df.boxplot(column='class', by='vgain', ax=axs[1])\n",
    "axs[1].set_title('Rise Time Distribution by Vgain')\n",
    "axs[1].set_xlabel('Vgain')\n",
    "axs[1].set_ylabel('Rise Time')\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Adjust layout and remove automatic figure title\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0d31a-fcc7-4f64-ab3b-152f5211cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "# Create subplots for pgain and vgain\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Box plot for pgain\n",
    "df.boxplot(column='class', by='motor', ax=axs[0])\n",
    "axs[0].set_title('Rise Time Distribution by Motor Type')\n",
    "axs[0].set_xlabel('Motor Type')\n",
    "axs[0].set_ylabel('Rise Time')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Box plot for vgain\n",
    "df.boxplot(column='class', by='screw', ax=axs[1])\n",
    "axs[1].set_title('Rise Time Distribution by Screw Type')\n",
    "axs[1].set_xlabel('Screw Type')\n",
    "axs[1].set_ylabel('Rise Time')\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Adjust layout and remove automatic figure title\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dff845-40df-4720-8beb-9035b547453b",
   "metadata": {},
   "source": [
    "### Comment on your findings below. What can you say about the shape, center, and spread of the distribution of Rise Times with respect to each feature? Which features have the most prominent outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa135d7-82c4-4cf7-ae09-6b7dac8a2eeb",
   "metadata": {},
   "source": [
    "(YOUR ANSWER GOES HERE.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15497a-85e2-4185-9823-4760061ba60f",
   "metadata": {},
   "source": [
    "### Task 4: One-Hot Encoding\n",
    "\n",
    "In order to include categorical variables like `motor` and `screw` types into a model that can only take in numerical inputs, we need to use **one-hot encoding**.\n",
    "\n",
    "Imagine we are trying to predict someone's final grade in CS 61A based on whether or not they passed or failed Midterm 1, using the regression model:\n",
    "\n",
    "$$\\text{Final Grade Prediction} = \\hat{y}_i = \\hat{\\alpha} + \\hat{\\beta} \\cdot \\text{Pass/Fail}_i$$\n",
    "    \n",
    "Pass/Fail here would be a categorical variable. However, we can treat this as sort of a \"boolean\" indicator variable, encoding the \"passes\" as 1 and the \"fails\" as 0. In other words, we can specify our prediction model as:\n",
    "\n",
    "$$\\text{Final Grade Prediction} = \\hat{y}_i = \\hat{\\alpha} + \\hat{\\beta} \\cdot \\{\\text{1 if Student Passed, 0 if Student Failed}\\}_i$$\n",
    "\n",
    "Notice here that we only needed **one indicator variable** to encode **two categories**. That's because the single indicator fully captures information about both categories, since if the indicator is 0 we know that the student failed the midterm. We don't need to add a separate indicator that equals 1 if the student failed because that would make our model redundant.\n",
    "\n",
    "You might argue that a binary Pass/Fail indicator is not complex enough of a predictor to capture patterns in a student's final grade. However, the 61A course staff informs you that due to privacy concerns, they will not release students' actual midterm scores for your model. So you take a different approach – you send out a survey with 3 options:\n",
    "- If a student did more than 5 points above the mean\n",
    "- If a student did more than 5 points below the mean\n",
    "- If a student got within 5 points of the mean\n",
    "\n",
    "Now we have 3 categories we need to encode. How do we do this? Well, recall that to encode two categories like Pass/Fail, we only needed 1 indicator to fully encode information about both categories. Similarly, to **encode 3 categories**, we only need **2 indicators** to fully capture all the information. Let's make this explicit with an example model:\n",
    "\n",
    "$$\\text{Final Grade Prediction} = \\hat{y}_i = \\hat{\\alpha} + \\hat{\\beta} \\cdot \\{\\text{1 if MT 1 Score > Mean + 5, 0 Otherwise}\\}_i + \\hat{\\gamma} \\cdot \\{\\text{1 if MT 1 Score < Mean - 5, 0 Otherwise}\\}_i$$\n",
    "\n",
    "If a student did not score more than 5 points above the mean, and the student did not score less than 5 points below the mean, then by process of elimination, we know that the student did within 5 points of the mean. Therefore, we don't need a third indicator variable to encode that information; it would make our model redundant.\n",
    "\n",
    "In general, this gets at a very important property of one-hot encoding: **if you are trying to encode $k$ categories, you only need $k - 1$ indicator variables (also called \"dummies\") to encode all of them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2284084-85aa-47fa-b184-5d88efeba427",
   "metadata": {},
   "source": [
    "Now for the coding task: use `pandas.get_dummies()` to do one-hot encoding for the categories in `motor` and `screw`:\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n",
    "\n",
    "Some important parameters to pay attention to:\n",
    "- the `prefix` parameter makes it so that when you generate the new indicator columns, their column names will start with whatever prefixes you assign, which makes it easier to refer to them later (you remember which original variable they were encoded from if you pass in the original column names, for example)\n",
    "- the `columns` parameter allows you to specify the names of the columns you want to one-hot encode (by default, if this isn't specified, it will encode all object/String/category data types, but in some cases we may want to encode numerically stored categories...which can be argued is the case for our problem, but we won't bother encoding those two columns)\n",
    "- the `drop_first` is **very important**! You should set this `=True` if you want to ensure you only get $k - 1$ dummies for the $k$ categories you are trying to encode. Otherwise, it will generate a column for each category, which, if you're not careful, may accidentally get passed into your model later.\n",
    "- the `dtype` parameter will allow you to save the newly encoded columns as `int`s or `float`s if you specify it as such; by default, it will create boolean indicator columns, which we don't necessarily want\n",
    "\n",
    "Save your newly one-hot encoded data as a dataframe called `design_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c13f4-bf0f-4bfc-8b33-e9d430fa3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_cols = ...\n",
    "design_matrix = ...\n",
    "design_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d02fe5-1020-48ea-be33-147b6fc4e163",
   "metadata": {},
   "source": [
    "### Task 5: Train-Test Split, Fit, Predict\n",
    "\n",
    "Say it with me everyone: in machine learning, the way we evaluate whether a model is \"good\" or not is **whether the model is able to generalize to data it hasn't seen before!**\n",
    "\n",
    "To create \"data the model hasn't seen before\", we need to create a **training set** – which is data the model will try to learn the underlying pattern behind (\"studying for the test\") – and a **test set** – which is data the model hasn't been trained on and therefore our way of knowing whether the model can predict well on data it hasn't seen before (\"taking the test\").\n",
    "\n",
    "Take a look at `sklearn`'s `train_test_split()` documentation.\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd736559-f239-4bc9-a03b-daff7e1896f2",
   "metadata": {},
   "source": [
    "#### Task 5.1 - Machine Learning Model with 1 Predictor\n",
    "\n",
    "Using just the `pgain` variable in `design_matrix`, your task is to\n",
    "- create a train-test split where `X_train` and `X_test` only contain `pgain`\n",
    "- use `sklearn`'s `LinearRegression` model to fit to the training data. Calculate the training root mean-squared error (RMSE).\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "- use your trained `LinearRegression` model to make predictions on the test data. Calculate the test root mean-squared error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d72b7-d787-4c9a-94ce-0e78e0bd23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_1 = ... # YOUR ANSWER GOES HERE - Select ONLY pgain from design_matrix\n",
    "... # YOUR ANSWER GOES HERE - Create a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf20a0-bc59-45c3-803d-c64a0af339dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = ... # YOUR ANSWER GOES HERE - Fit a linear model using sklearn\n",
    "y_train_preds = # YOUR ANSWER GOES HERE - Make predictions on the TRAINING set (we will use this later to get the training RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58919b-8d2e-43b5-baa7-d5e69d5f308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(y_true, y_preds):\n",
    "    return ... # YOUR ANSWER GOES HERE - Complete the RMSE Function\n",
    "\n",
    "training_error = calculate_rmse(...) # YOUR ANSWER GOES HERE - What should we pass in to get training RMSE?\n",
    "print(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1baf99-1dcd-4831-a29d-ff4c23add5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = ... # YOUR ANSWER GOES HERE - Make predictions on the TEST set (we will use this later to get the test RMSE)\n",
    "test_error = ... # YOUR ANSWER GOES HERE - Calculate the test RMSE\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a1122-8222-4e28-afb3-d811c87f333f",
   "metadata": {},
   "source": [
    "#### Task 5.2 - Machine Learning Model with 2 Predictors\n",
    "\n",
    "Repeat your steps in Task 5.1, except your design matrix of features should include both `pgain` and `vgain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b96d59-029a-4850-ba70-67a750cc16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = ... # YOUR ANSWER GOES HERE - Select pgain and vgain from design_matrix\n",
    "... # YOUR ANSWER GOES HERE - Create a new train-test split with these two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825da78-90d3-436e-9e68-9bc91c62f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = ... # YOUR ANSWER GOES HERE - Fit a linear model using sklearn\n",
    "y_train_preds = ... # YOUR ANSWER GOES HERE - Make predictions on the TRAINING set (we will use this later to get the training RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c3dda-54aa-433a-ab71-e6bb6b2e4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = ... # YOUR ANSWER GOES HERE - Calculate the training RMSE\n",
    "print(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae80ef-d115-4ed4-bab8-bd84393ee6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds =  ... # YOUR ANSWER GOES HERE - Make predictions on the TEST set\n",
    "test_error = ... # YOUR ANSWER GOES HERE - Calculate the test RMSE\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa84cb8-9cb1-46f7-b15c-7761fe654b8e",
   "metadata": {},
   "source": [
    "#### Task 5.4 - Machine Learning Model with All Predictors\n",
    "\n",
    "Repeat your steps in Tasks 5.1 and 5.2, but this time include all the feature – including the one-hot encoded categorical features – in the `design_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0419c-bd10-44ab-bc6e-4ec929f00590",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # YOUR ANSWER GOES HERE - Create a train-test split using all features in design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb9ae0-2a26-46ff-9790-7f0868165d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = ... # YOUR ANSWER GOES HERE - Fit a linear model using sklearn\n",
    "y_train_preds = ... # YOUR ANSWER GOES HERE - Make predictions on the TRAINING set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbce85b-bd1a-4b17-939d-33dff5e31f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = ... # YOUR ANSWER GOES HERE - Calculate the training RMSE\n",
    "print(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347bb0d-2e29-4bb2-9cde-d4a86942a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = ... # YOUR ANSWER GOES HERE - Make predictions on the TEST set\n",
    "test_error = ... # YOUR ANSWER GOES HERE - Calculate the test RMSE\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a405f7-25c7-45f6-82c6-71f53130ae57",
   "metadata": {},
   "source": [
    "#### Task 5.5 - What do you notice about the training and testing error as we added more features? Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf6028-048f-4eda-b42a-89248150bf22",
   "metadata": {},
   "source": [
    "(YOUR ANSWER GOES HERE.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0f5b7-c751-4eec-a729-7143ce8c514b",
   "metadata": {},
   "source": [
    "### Task 6 - An Experiment in Nonsense\n",
    "\n",
    "By the almighty principle of induction, we might be inclined to believe that adding more and more features will keep reducing our training and test error? But is that true? Let's verify it empirically.\n",
    "\n",
    "For this task, work in groups to do the following:\n",
    "- set a random seed of 1 using `np.random.seed(1)`\n",
    "- create two lists or arrays to hold all your training and test RMSEs\n",
    "- create a feature with randomly generated data. How you randomly generate this data is up to you; it doesn't really matter.\n",
    "    - one way to generate random data is to draw samples from a standard normal distribution using `np.random.normal()`: https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html\n",
    "    - again, you need not use a normal distribution. You can randomly generate any numbers you want in any manner you want.\n",
    "    - make sure your randomly generated feature has the same length as the columns of the design matrix!\n",
    "- repeat the steps from Task 5 (train-test split, fit, predict) and save your training RMSEs and testing RMSEs in your two lists\n",
    "- repeat this procedure $n - p$ times, where $n$ is the number of observations in `X_train` and $p$ is the number of features that already exist in `design_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71da86-5791-4bdc-9519-463ba3bb25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # YOUR ANSWER GOES HERE - Set a random seed of 1\n",
    "training_rmses = ... # YOUR ANSWER GOES HERE - Create an empty list or array to hold training RMSEs\n",
    "testing_rmses = ... # YOUR ANSWER GOES HERE - Create an empty list or array to hold test RMSEs\n",
    "\n",
    "\n",
    "X_train_master, X_test, y_train, y_test = train_test_split(design_matrix, y, test_size=0.33, random_state=42)\n",
    "\n",
    "p = len(X_train_master.columns)\n",
    "design_matrix_copy = design_matrix.copy() # USE THIS COPIED DATAFRAME design_matrix_copy\n",
    "\n",
    "for i in range(len(X_train_master) - p):\n",
    "    random_feature = ... # YOUR ANSWER GOES HERE - Create a random feature of your choice\n",
    "    design_matrix_copy[f\"rand_{i+1}\"] = random_feature # We add this random feature to a new column\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = ... # YOUR ANSWER GOES HERE - Create a new train-test split with this added feature!\n",
    "    \n",
    "    reg = ... # YOUR ANSWER GOES HERE - Initialize and fit a linear model from sklearn\n",
    "    y_train_preds = ... # YOUR ANSWER GOES HERE - Make predictions on the training set\n",
    "    training_error = ... # YOUR ANSWER GOES HERE - Calculate training RMSE\n",
    "    y_test_preds = ... # YOUR ANSWER GOES HERE - Make predictions on the test set\n",
    "    test_error = ... # YOUR ANSWER GOES HERE - Calculate test RMSE\n",
    "    \n",
    "    ... # YOUR ANSWER GOES HERE - Append this iteration's training error to your original list or array\n",
    "    ... # YOUR ANSWER GOES HERE - Append this iteration's test error to your original list or array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb813d-ee33-4ff8-9476-444446aec3d9",
   "metadata": {},
   "source": [
    "Now, let's create a plot of our training error over each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107cebc9-523b-417f-83d3-2f02a0773873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "x_values = list(range(11, 11 + len(training_rmses)))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, training_rmses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training Error Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99fe8f-efa7-4409-8ac1-5551866b0766",
   "metadata": {},
   "source": [
    "Now, let's do the same for the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec4d24-7b65-4606-889c-4a980ef2821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "x_values = list(range(11, 11 + len(testing_rmses)))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, testing_rmses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Test Error')\n",
    "plt.title('Test Error Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a851cb4-7d36-436f-b1aa-57d65f4d7b41",
   "metadata": {},
   "source": [
    "### What do you observe? What is this phenomenon called? Why is it happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032dd099-2da3-4940-b493-ee7eab3e2583",
   "metadata": {},
   "source": [
    "(YOUR ANSWER GOES HERE.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce360b-15b1-44cf-8106-5a8c3dbd5d94",
   "metadata": {},
   "source": [
    "### Task 7 - Bias-Variance Tradeoff\n",
    "\n",
    "You might often see overfitting and underfitting discussed in the context of the **bias-variance tradeoff**, which helps explain how model complexity affects prediction error. \n",
    "\n",
    "- When a model is too simple (low complexity), it has **high bias**. It makes strong assumptions about the data and fails to capture patterns, leading to underfitting.\n",
    "- On the other hand, when a model is too complex, it has **high variance**. It captures noise along with the true pattern, making it sensitive to small fluctuations in the training data and leading to overfitting.\n",
    "- The total prediction error consists of bias, variance, and irreducible error. As we add more features or increase model complexity, bias decreases (model fits the data better), but variance increases (model becomes more sensitive to fluctuations).\n",
    "\n",
    "The goal is to find the optimal model complexity where both bias and variance are balanced, minimizing total error.\n",
    "\n",
    "![Bias Variance Tradeoff](bias_variance_tradeoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777969a-7ffa-4f0b-ab17-98ab5ecbbd86",
   "metadata": {},
   "source": [
    "### YOUR TASK: Explain the bias variance tradeoff in the context of our experiment in Task 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c481f6-3359-4d96-ba26-c757c9b600e1",
   "metadata": {},
   "source": [
    "(YOUR ANSWER GOES HERE.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0484b-12d8-4754-aeba-d6e9fc0dca64",
   "metadata": {},
   "source": [
    "### Task 8: Regularization to the Rescue!\n",
    "\n",
    "Overfitting is highly problematic for machine learning models; it goes against our main criteria to decide whether a model is \"good\" or not since the model cannot generalize outside of the data it was trained on. As we have seen, when models become too complex with too many parameters, this can cause the model to learn the irrelevant noise in the training data rather than focus on the general pattern.\n",
    "\n",
    "One way to combat this is **regularization**, which can be thought of as **making our complex model simpler**. Regularization achieves this by adding a constraint on the parameters (coefficients) of the model and penalizing their magnitudes if they get too large. That way, for our nonsense features created above, their coefficients would be shrunk towards 0 so they do not have has large an impact on the model.\n",
    "\n",
    "Formally, a regularized linear regression model over $n$ observations and $p$ predictors takes the form:\n",
    "\n",
    "$$\\text{Least Squares Problem} + \\alpha * \\text{(Regularization ``Penalty\" Term)} = \\frac{1}{n} \\sum_{i=1}^n (y_i - (\\sum_{j=1}^p \\beta_j x_{ij}))^2 + \\alpha \\|\\beta\\|_P^P$$\n",
    "\n",
    "where $\\alpha$ controls the **strength** of the penalty.\n",
    "- High $\\alpha$ values have **larger** penalties on the coefficients and shrink more of them\n",
    "- Low $\\alpha$ values have **smaller** penalties on the coefficients and don't reduce them as much\n",
    "\n",
    "In the context of optimization, you can think of the least squares problem and the penalty term as **playing a game** against each other. The least square problem really wants to minimize the mean-squared error to get as close as possible to the actual true coefficient values (low bias). The penalty term hates the mean-squared error, and isn't happy when the coefficients are close to the true values. To make both the least squares term and the penalty term happy, the solution to the regularized model will keep the most important least-squares coefficients, but shrink the unimportant or nonsense coefficients down to 0. In effect, we're introducing a little bit of **bias** into the model parameters, but with the benefit that it will **reduce the variance** and help the model generalize well outside the training data with reduced complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b20dbf-f7b0-4a3f-8504-0bd0ee425b01",
   "metadata": {},
   "source": [
    "#### 8.1 - Exploring $\\ell^P$-Norms\n",
    "\n",
    "One thing you may have noticed in the penalty term of our regularized linear model is the $\\|\\beta\\|_P$ term. This is called an **$\\ell^P$-norm**.\n",
    "- I'm using an uppercase P to avoid confusion with $p$ which is the number of features in our model, but you may see it written as lowercase $\\ell^p$-norm in a linear algebra textbook.\n",
    "\n",
    "Generally, the $\\ell^P$ norm of a vector $\\beta$ takes the form $\\|\\beta\\|_P = (|\\beta_1|^P + |\\beta_2|^P + ... + |\\beta_p|^P)^{1/P}$\n",
    "\n",
    "This idea leads to two **very important** types of regularization for linear models:\n",
    "\n",
    "#### (1) The $\\ell^1$-Norm, aka **LASSO Regression**\n",
    "\n",
    "When P = 1, $\\|\\beta\\|_P^P = \\|\\beta\\|_1^1 = ((|\\beta_1|^1 + |\\beta_2|^1 + ... + |\\beta_p|^1)^{1/1})^1 = \\sum_{j=1}^p |\\beta_j|$\n",
    "\n",
    "As we can see, **LASSO Regression** imposes a penalty on the coefficients of our model by summing over their absolute values. Let's see what this constraint looks like in two-dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a250908-e04f-4e78-a6ba-644b45492ec0",
   "metadata": {},
   "source": [
    "First, let's run a least-squares regression on some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0fd57-c6c0-46e7-8c06-b7c3a16ad2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Create two correlated features\n",
    "X1 = np.random.randn(n_samples)\n",
    "X2 = 0.5 * X1 + np.random.randn(n_samples) * 0.1  # Slight correlation\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Generate target variable with only X1 contributing significantly\n",
    "beta_true = np.array([3, 0])  # Only first feature is important\n",
    "y = X @ beta_true + np.random.randn(n_samples) * 10\n",
    "\n",
    "# Fit Ordinary Least Squares (OLS) Regression\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X, y)\n",
    "ols_coeffs = ols_model.coef_\n",
    "print(ols_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a3c0e-29ae-4f84-b9ea-2f7fde9bf4ac",
   "metadata": {},
   "source": [
    "Let's plot these points on our 2D Plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6e320-2c3e-4383-8758-3ad0d72e340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(ols_coeffs[0], ols_coeffs[1], label='Least Squares Solution')\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "plt.xlabel(r'$\\beta_1$ (Coefficient for $X_1$)')\n",
    "plt.ylabel(r'$\\beta_2$ (Coefficient for $X_2$)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb2953-d606-44cf-8860-b425612e4c03",
   "metadata": {},
   "source": [
    "Now, let's add our $\\ell^1$ constraint. In the x-y plane, this will take the form:\n",
    "\n",
    "$$(|x|^P + |y|^P)^{1/P} = |x| + |y|$$ \n",
    "\n",
    "since P = 1. Before running the code below, what shape will this generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b896c-05e0-4621-9479-0323e1fbc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot centered at the origin with L1 constraint\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the Least Squares Solution\n",
    "plt.scatter(ols_coeffs[0], ols_coeffs[1], label='Least Squares Solution')\n",
    "\n",
    "# Add L1 constraint\n",
    "diamond_x = np.array([1, 0, -1, 0, 1]) * 4.05377455\n",
    "diamond_y = np.array([0, 1, 0, -1, 0]) * 4.05377455\n",
    "plt.plot(diamond_x, diamond_y, color='gray', label=r'L1 Constraint ($\\ell^1$-norm)')\n",
    "\n",
    "# Set axis limits to match the given plot\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "\n",
    "# Add grid, labels, and legend\n",
    "plt.xlabel(r'$\\beta_1$ (Coefficient for $X_1$)')\n",
    "plt.ylabel(r'$\\beta_2$ (Coefficient for $X_2$)')\n",
    "plt.title('Least Squares Solution and L1 Constraint')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124470c0-3b16-4aa1-a267-45240458c362",
   "metadata": {},
   "source": [
    "What the $\\ell^1$ constraint is essentially doing is creating a region along which the penalized LASSO solution must occur. Basically, all solutions subject to the $\\ell^1$ penalty are constrained to that region on the x-y plane.\n",
    "\n",
    "Now, let's try fitting a **LASSO regression** using `sklearn`. Documentation is available below, but for today's lab, we've done it for you!\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddfd71-1614-4b5d-98c2-9d6ed9411089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_model = Lasso(alpha=1)\n",
    "lasso_model.fit(X, y)\n",
    "lasso_coeffs = lasso_model.coef_\n",
    "print(lasso_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95a81c-bfb3-49bf-8e24-e0934cb742ac",
   "metadata": {},
   "source": [
    "**What do you notice about one of the coefficients?**\n",
    "\n",
    "Let's plot this point along our LASSO constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc500d-6ffd-4a88-8a6d-1acfc7a25b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "# Create the plot centered at the origin with L1 constraint\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the Least Squares Solution\n",
    "plt.scatter(ols_coeffs[0], ols_coeffs[1], label='Least Squares Solution')\n",
    "\n",
    "# Plot the LASSO Solution\n",
    "plt.scatter(lasso_coeffs[0], lasso_coeffs[1], label='LASSO Solution')\n",
    "\n",
    "# Add L1 constraint (diamond shape)\n",
    "l1_radius = np.sum(np.abs(lasso_coeffs))  # Compute the L1 norm of LASSO coefficients\n",
    "diamond_x = np.array([1, 0, -1, 0, 1]) * l1_radius\n",
    "diamond_y = np.array([0, 1, 0, -1, 0]) * l1_radius\n",
    "plt.plot(diamond_x, diamond_y, color='gray', label=r'L1 Constraint ($\\ell^1$-norm)')\n",
    "\n",
    "# Set axis limits to match the given plot\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "\n",
    "# Add grid, labels, and legend\n",
    "plt.xlabel(r'$\\beta_1$ (Coefficient for $X_1$)')\n",
    "plt.ylabel(r'$\\beta_2$ (Coefficient for $X_2$)')\n",
    "plt.title('Least Squares Solution and L1 Constraint')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66275f-a39b-430e-8cd1-79a358e4d005",
   "metadata": {},
   "source": [
    "Let's see what happens when we increase the value of `alpha` – or, in other words, strengthen our penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d2e9d-b643-475d-a473-c5753119253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad1ab2-be1f-4207-91b7-efe97910749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "lasso_model.fit(X, y)\n",
    "lasso_coeffs = lasso_model.coef_\n",
    "print(lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03885504-c6ca-4ac8-b782-584f9ff1568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "# Create the plot centered at the origin with L1 constraint\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the Least Squares Solution\n",
    "plt.scatter(ols_coeffs[0], ols_coeffs[1], label='Least Squares Solution')\n",
    "\n",
    "# Plot the LASSO Solution\n",
    "plt.scatter(lasso_coeffs[0], lasso_coeffs[1], label='LASSO Solution')\n",
    "\n",
    "# Add L1 constraint (diamond shape)\n",
    "l1_radius = np.sum(np.abs(lasso_coeffs))  # Compute the L1 norm of LASSO coefficients\n",
    "diamond_x = np.array([1, 0, -1, 0, 1]) * l1_radius\n",
    "diamond_y = np.array([0, 1, 0, -1, 0]) * l1_radius\n",
    "plt.plot(diamond_x, diamond_y, color='gray', label=r'L1 Constraint ($\\ell^1$-norm)')\n",
    "\n",
    "# Set axis limits to match the given plot\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "\n",
    "# Add grid, labels, and legend\n",
    "plt.xlabel(r'$\\beta_1$ (Coefficient for $X_1$)')\n",
    "plt.ylabel(r'$\\beta_2$ (Coefficient for $X_2$)')\n",
    "plt.title('Least Squares Solution and L1 Constraint')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401f78d-e139-4d3c-bdab-b29e3bea7279",
   "metadata": {},
   "source": [
    "What do you observe about our LASSO solution and our constrained region?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c8f99-5073-4d05-8bd1-3c7d8a5bee5c",
   "metadata": {},
   "source": [
    "#### (2) The $\\ell^2$-Norm, aka **Ridge Regression**\n",
    "\n",
    "LASSO helped us learn what happens when we set P = 1, but really, there's nothing stopping us from increasing P to arbitrary powers.\n",
    "\n",
    "In addition to LASSO, which is the $\\ell^1$-Norm, the most common type of regularization function is the **Ridge penalty** or the $\\ell^2$-Norm.\n",
    "\n",
    "When P = 2, $\\|\\beta\\|_P^P = \\|\\beta\\|_2^2 = ((|\\beta_1|^2 + |\\beta_2|^2 + ... + |\\beta_p|^2)^{1/2})^2 = (\\sqrt{\\sum_{j=1}^p \\beta_j^2})^2 = \\sum_{j=1}^p \\beta_j^2$\n",
    "\n",
    "As we can see, **Ridge Regression** imposes a penalty on the coefficients of our model by summing over their squares.\n",
    "\n",
    "Let's revisit our plots from above, but this time, let's fit a **Ridge regression** to our data and see how it compares to the least squares estimate.\n",
    "- `sklearn`'s Ridge documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a795ba1-b778-4d98-b403-83f1617b9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=1)\n",
    "ridge_model.fit(X, y)\n",
    "ridge_coeffs = ridge_model.coef_\n",
    "print(ridge_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e0b98-def0-47bb-a616-bb998ba26c4f",
   "metadata": {},
   "source": [
    "How do these coefficients compare to the LASSO coefficients for the same strength of regularization $\\alpha$?\n",
    "\n",
    "Before we plot the $\\ell^2$ constraint in the x-y plane, what do you think the constrained space will look like? In other words, what will\n",
    "\n",
    "$$(x^P + y^P)^{1/P}$$\n",
    "\n",
    "look like for P = 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347b865-3209-4066-8fb3-4d3f7e19fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "# Create the plot centered at the origin with L1 constraint\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the Least Squares Solution\n",
    "plt.scatter(ols_coeffs[0], ols_coeffs[1], label='Least Squares Solution')\n",
    "\n",
    "# Plot the Ridge Solution\n",
    "plt.scatter(ridge_coeffs[0], ridge_coeffs[1], label='Ridge Solution')\n",
    "\n",
    "# Add L2 constraint (circle shape)\n",
    "theta = np.linspace(0, 2 * np.pi, 100)  # 100 points around the circle\n",
    "l2_radius = np.sqrt(np.sum(ridge_coeffs ** 2))  # Compute the L2 norm of Ridge coefficients\n",
    "circle_x = l2_radius * np.cos(theta)  # Parametric equation for a circle\n",
    "circle_y = l2_radius * np.sin(theta)\n",
    "plt.plot(circle_x, circle_y, color='gray', label=r'L2 Constraint ($\\ell^2$-norm)')\n",
    "\n",
    "# Set axis limits to match the given plot\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "\n",
    "# Add grid, labels, and legend\n",
    "plt.xlabel(r'$\\beta_1$ (Coefficient for $X_1$)')\n",
    "plt.ylabel(r'$\\beta_2$ (Coefficient for $X_2$)')\n",
    "plt.title('Least Squares Solution and L2 Constraint')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e2601-8679-4e3f-9572-abf15cd5cca8",
   "metadata": {},
   "source": [
    "Now, let's once again increase the strength of the regularization parameter, and see what happens both to our constrained space and the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee04eb-94b2-4bdb-a1f3-2aafc070dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fd98e-d506-40d7-897b-47ae81a3dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X, y)\n",
    "ridge_coeffs = ridge_model.coef_\n",
    "print(ridge_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed9b2b-b360-42c9-aa3f-2b29cdd0842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "# Create the plot centered at the origin with L1 constraint\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the Least Squares Solution\n",
    "plt.scatter(ols_coeffs[0], ols_coeffs[1], label='Least Squares Solution')\n",
    "\n",
    "# Plot the Ridge Solution\n",
    "plt.scatter(ridge_coeffs[0], ridge_coeffs[1], label='Ridge Solution')\n",
    "\n",
    "# Add L2 constraint (circle shape)\n",
    "theta = np.linspace(0, 2 * np.pi, 100)  # 100 points around the circle\n",
    "l2_radius = np.sqrt(np.sum(ridge_coeffs ** 2))  # Compute the L2 norm of Ridge coefficients\n",
    "circle_x = l2_radius * np.cos(theta)  # Parametric equation for a circle\n",
    "circle_y = l2_radius * np.sin(theta)\n",
    "plt.plot(circle_x, circle_y, color='gray', label=r'L2 Constraint ($\\ell^2$-norm)')\n",
    "\n",
    "# Set axis limits to match the given plot\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "\n",
    "# Add grid, labels, and legend\n",
    "plt.xlabel(r'$\\beta_1$ (Coefficient for $X_1$)')\n",
    "plt.ylabel(r'$\\beta_2$ (Coefficient for $X_2$)')\n",
    "plt.title('Least Squares Solution and L2 Constraint')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b999b-9c8f-49e3-963c-3a3915859a04",
   "metadata": {},
   "source": [
    "Compare what you are noticing for different values of `alpha` in the Ridge model to what you noticed with different values of `alpha` in the LASSO model.\n",
    "\n",
    "In general, it takes **much stronger regularization** to shrink the coefficients in the Ridge model compared to LASSO. Additionally, it's much easier for the LASSO model to have coefficients that shrink all the way down to 0, whereas even for large `alpha` in the Ridge model, our coefficients only approach, but never actually reach, 0.\n",
    "\n",
    "This actually teaches a very valuable lesson in the differences between LASSO and Ridge.\n",
    "- Because LASSO uses an $\\ell^1$ penalty, it is able to induce **sparsity** in its coefficients. In other words, LASSO is able to shrink coefficients all the way down to **exactly 0**.\n",
    "    - The additional benefit of this is that LASSO also acts as a form of **feature selection** for us! If we are trying to decide which features are important and which aren't, we can use LASSO to shrink the least important features down to 0 and just select a subset of the most important ones.\n",
    "\n",
    "- Ridge, on the other hand, uses an $\\ell^2$ penalty. Because of this, it is much harder for solutions to the Ridge regression to hit exactly 0 on either of the parameters. For high regularization strength, Ridge will shrink the parameters such that they **approach 0**, but they will never actually *equal* 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033f300-9c5c-4384-a7f4-649a66ebf368",
   "metadata": {},
   "source": [
    "### Task 9: Revisiting Servo with Regularization\n",
    "\n",
    "Let's go back to our Servo dataset at the beginning of the lab and explore what happens to our models if we use regularization to learn the optimal parameters instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b132c94-d698-4baf-8618-d33e236ce611",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e93281-ae27-4861-9790-5f8956d7c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL - LASSO Model\n",
    "X = servo.data.features \n",
    "y = servo.data.targets \n",
    "encode_cols = [\"motor\", \"screw\"]\n",
    "design_matrix = pd.get_dummies(X, prefix=encode_cols, columns=encode_cols, drop_first=True, dtype=int)\n",
    "np.random.seed(1)\n",
    "training_rmses = []\n",
    "testing_rmses = []\n",
    "X_train_master, X_test, y_train, y_test = train_test_split(design_matrix, y, test_size=0.33, random_state=42)\n",
    "p = len(X_train_master.columns)\n",
    "design_matrix_copy = design_matrix.copy()\n",
    "for i in range(len(X_train_master) - p):\n",
    "    random_feature = np.random.normal(size=len(design_matrix_copy))\n",
    "    design_matrix_copy[f\"rand_{i+1}\"] = random_feature\n",
    "    X_train, X_test, y_train, y_test = train_test_split(design_matrix_copy, y, test_size=0.33, random_state=42)\n",
    "    reg = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "    y_train_preds = reg.predict(X_train)\n",
    "    training_error = calculate_rmse(y_train[\"class\"], y_train_preds)\n",
    "    y_test_preds = reg.predict(X_test)\n",
    "    test_error = calculate_rmse(y_test[\"class\"], y_test_preds)\n",
    "    \n",
    "    training_rmses.append(training_error)\n",
    "    testing_rmses.append(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1434ff-9d16-48cc-b914-79da98474a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd744fb-e194-4d57-986b-f2a0078d82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "x_values = list(range(11, 11 + len(training_rmses)))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, training_rmses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training Error Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cca97d-24b9-4b04-ab43-d6b09a30304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "x_values = list(range(11, 11 + len(testing_rmses)))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, testing_rmses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Test Error')\n",
    "plt.title('Test Error Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107be422-80da-44fb-bd69-9d4ef949a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8095c43-ad61-4333-ac42-242c43f0f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL - Ridge Model\n",
    "X = servo.data.features \n",
    "y = servo.data.targets \n",
    "encode_cols = [\"motor\", \"screw\"]\n",
    "design_matrix = pd.get_dummies(X, prefix=encode_cols, columns=encode_cols, drop_first=True, dtype=int)\n",
    "np.random.seed(1)\n",
    "training_rmses = []\n",
    "testing_rmses = []\n",
    "X_train_master, X_test, y_train, y_test = train_test_split(design_matrix, y, test_size=0.33, random_state=42)\n",
    "p = len(X_train_master.columns)\n",
    "design_matrix_copy = design_matrix.copy()\n",
    "for i in range(len(X_train_master) - p):\n",
    "    random_feature = np.random.normal(size=len(design_matrix_copy))\n",
    "    design_matrix_copy[f\"rand_{i+1}\"] = random_feature\n",
    "    X_train, X_test, y_train, y_test = train_test_split(design_matrix_copy, y, test_size=0.33, random_state=42)\n",
    "    reg = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    y_train_preds = reg.predict(X_train)\n",
    "    training_error = calculate_rmse(y_train[\"class\"], y_train_preds)\n",
    "    y_test_preds = reg.predict(X_test)\n",
    "    test_error = calculate_rmse(y_test[\"class\"], y_test_preds)\n",
    "    \n",
    "    training_rmses.append(training_error)\n",
    "    testing_rmses.append(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebc0a0-6b32-4bfb-a422-91aa47a95587",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31897d88-d6e7-4e9b-a6e6-d3e8ff2a2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "x_values = list(range(11, 11 + len(training_rmses)))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, training_rmses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training Error Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30154348-3083-4ecd-b59c-ed0c65750d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "x_values = list(range(11, 11 + len(testing_rmses)))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, testing_rmses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Test Error')\n",
    "plt.title('Test Error Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e36f4d-c613-4c0c-b374-65e5980b57ff",
   "metadata": {},
   "source": [
    "What we've done here is **significantly reduced the variance** of our model by just adding a little bit of bias into our estimated model parameters. In exchange for a little bit of bias, we've greatly reduced the degree to which our model overfits, even for a large number of nonsense features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1854c8d-e1cb-45a6-8ba9-2e48f04d8179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
