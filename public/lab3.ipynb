{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 3: Exploratory Data Analysis and Data Cleaning**\n",
        "\n",
        "Today, we are going to be conducting **exploratory data analysis (EDA)** on a dataset of trending YouTube videos. We will be creating key data visualizations, summarizing our findings with descriptive statistics, and trying to identify which groups or topics of videos perform best and worst!"
      ],
      "metadata": {
        "id": "yc28GC3AS4yF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import our key libraries. We've already seen `pandas` and `numpy` extensively, but today we will be working with `matplotlib` and `seaborn` for the first time. These libraries allow us to create data visualizations."
      ],
      "metadata": {
        "id": "eco_ar8STRwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Caudoava55Ri"
      },
      "outputs": [],
      "source": [
        "# JUST RUN THIS CELL\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are also giving you a crude subtopic modeling function, which leverages a **transformer** model that will group YouTube videos into subtopics based on title and channel name.\n",
        "\n",
        "Note that this cell may take around 30 seconds to run so do not fret!"
      ],
      "metadata": {
        "id": "2RIK5WQPTcXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"cross-encoder/nli-deberta-v3-small\")\n",
        "\n",
        "subtopic_labels = [\"trump\", \"clinton\", \"nba\", \"roy moore\", \"net neutrality\", \"ajit pai\", \"elections\", \"republicans\", \"democrats\", \"special needs\",\n",
        "                   \"movie trailer\", \"tina nguyen\", \"lawsuits\", \"activism\", \"book review\", \"supreme court\", \"jimmy kimmel\", \"youtube drama\", \"celebrity news\"]\n",
        "\n",
        "def get_subtopics(title: str, subtopic_labels: list) -> str:\n",
        "    result = classifier(title, subtopic_labels)\n",
        "    return result[\"labels\"][0]  # Return highest confidence label\n",
        "\n",
        "get_subtopics(\"Insane Three-Point Shot Made by Stephen Curry! | Lakers vs. Warriors Basketball Highlights NBA\", subtopic_labels)"
      ],
      "metadata": {
        "id": "DubvJa-CHFWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Load in the Data\n",
        "\n",
        "The dataset will be at the following URL below:"
      ],
      "metadata": {
        "id": "fCwLca8sT1vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "URL = \"https://raw.githubusercontent.com/ArnavG/saas_workshops/refs/heads/main/USvideos.csv.zip\""
      ],
      "metadata": {
        "id": "u8KebFBvT58P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `pandas.read_csv()` to open the zip file in the URL, just like you always have! (Under the hood, the function is inferring that the file type is a zip file and passing in arguments to handle this file type. In other cases, we might not get so lucky.)"
      ],
      "metadata": {
        "id": "HmY9sMh-T9DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = ...\n",
        "df"
      ],
      "metadata": {
        "id": "MhK8ud8P8-tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Check which Columns have Null or Missing Values\n",
        "\n",
        "- **Hint**: use `pandas.DataFrame.isna()` and a certain aggregation function\n",
        "  - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html"
      ],
      "metadata": {
        "id": "XBBXUJADUVnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "PnAP_HLE91ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 - DateTime Conversion\n",
        "\n",
        "We have an existing DateTime column called `publish_time`, which tells us when the video was uploaded to YouTube, but doesn't tell us when the video actually started trending. For that, we need to look at the `trending_date` column.\n",
        "\n",
        "To convert each date in `trending_date` to a DateTime object, we can use the following code syntax:"
      ],
      "metadata": {
        "id": "UlnxZaD8MYYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "\n",
        "date_str = \"17.14.11\" # Sample date\n",
        "date_obj = pd.to_datetime(date_str, format=\"%y.%d.%m\") # Convert to datetime (YY.DD.MM)\n",
        "print(date_obj)"
      ],
      "metadata": {
        "id": "W-QWjsXuMeO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, convert all the dates in `trending_date` into DateTime format. You may either use `pandas.Series.apply()` or take advantage of vectorized operations.\n",
        "- https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html"
      ],
      "metadata": {
        "id": "01ae207fMjgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"trending_date\"] = ...\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zFNCeuqZMpo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll create a new column called `trending_date_year_month` that takes the DateTime object in the `trending_date` column and converts each one into a `YYYY-MM` format, like so:"
      ],
      "metadata": {
        "id": "gh7EP29fM0Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "df[\"trending_date_year_month\"] = df[\"trending_date\"].dt.strftime('%Y-%m')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HWD4diGWM1Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4 - Aggregating the Data\n",
        "\n",
        "Sometimes, the same YouTube video will appear on the trending page multiple times over the course of many days. It may be off the trending page at one point, then find its way back on, then off, then on again, etc.\n",
        "\n",
        "This complicates things for our analysis because the same video will appear with different view, like, dislike, and comment counts, multiple times in the dataset. However, we can't simply take the sum of these engagement metrics, because they represent the total engagement numbers at that moment in time, not the changes since the last time the video was trending.\n",
        "\n",
        "For example, take the video titled `\"Judge Roy Moore Campaign Statement\"`"
      ],
      "metadata": {
        "id": "LNSgvQlRIPKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "df[df[\"title\"] == \"Judge Roy Moore Campaign Statement\"]"
      ],
      "metadata": {
        "id": "6FtxcHY0ITav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It first hit trending with on December 14, 2017, with 51,139 views, 441 likes, and 4693 dislikes, but over the course of one week, it accumulated more and more views, likes, and dislikes (especially dislikes). Comments appear to have been disabled. (You should go look up who Roy Moore is if you are unfamiliar to find out why this might be.)\n",
        "\n",
        "To get the number of views, likes, dislikes, and comments for each video, we need to group by video title, but instead of aggregating by **summing** the engagement metrics, we need to use another aggregation function. What should this function be?\n",
        "\n",
        "1. Group the `df` dataframe by all **categorical features** and find the final number of views,likes, dislikes, and comments that each video had.\n",
        "2. Do not omit any columns.\n",
        "3. Save this grouped dataframe as `trended_videos`. Chain the `reset_index()` function to the end of your code as well."
      ],
      "metadata": {
        "id": "Lkkmt9onIoip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1 - Create a categorical features list\n",
        "# You can either hard-code this, or you can get creative and use .dtypes and boolean filtering to get them. I would recommend the latter as a good brain teaser\n",
        "categorical_columns = ...\n",
        "categorical_columns"
      ],
      "metadata": {
        "id": "CabeE6_lKnMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2 - Create the trended_videos dataframe with the correct groupby() syntax\n",
        "trended_videos = ...\n",
        "trended_videos"
      ],
      "metadata": {
        "id": "I815o6XxKM5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "trended_videos[trended_videos[\"title\"] == \"Judge Roy Moore Campaign Statement\"]"
      ],
      "metadata": {
        "id": "08qMfE5ZLPqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does this match what you expected? If not, double check your grouping and aggregation functions.\n",
        "\n",
        "We will be working with the `trended_videos` dataframe for subequent tasks as well."
      ],
      "metadata": {
        "id": "NK_v6HYULZfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5 - The YouTube Channels that Trended the Most\n",
        "\n",
        "\n",
        "### 5.1\n",
        "Get the **number of times** each YouTube video in the dataset appeared on the trending page. Sort this dataframe by the number of trending page appearances from greatest to least. Store the result in a dataframe called `trending_counts`.\n",
        "\n",
        "- **Hint**: use `pandas.DataFrame.groupby()`\n",
        "  - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
        "- **Hint**: think about what aggregation function you will need to use to get the *number* of times each title appeared in the trending dataset (if you are stuck, ask for help!)\n",
        "- **Hint**: use `pandas.DataFrame.sort_values()`\n",
        "  - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html"
      ],
      "metadata": {
        "id": "DDEAiDmWUs_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trending_counts = ...\n",
        "trending_counts"
      ],
      "metadata": {
        "id": "GdKIlPyO-qTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2\n",
        "Create a histogram of the number of times each channel appeared on the trending page. You may either use:\n",
        "\n",
        "- `matplotlib.pyplot.hist()`\n",
        "  - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
        "\n",
        "or\n",
        "\n",
        "- `seaborn.histplot()`\n",
        "  - https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "\n",
        "to create your histogram. Be mindful of the parameters you pass in. Feel free to search up other plotting documentation or examples. I would also recommend sending the parameter `kde=True`"
      ],
      "metadata": {
        "id": "A6xFJtRlVhOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "Y7vGzKve_USq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3\n",
        "\n",
        "Let's generate some summary statistics for this trending counts distribution. Print out the following information:\n",
        "\n",
        "- The **minimum** of the distribution (hint: `numpy.min()`)\n",
        "- The **1st percentile** of the distribution (hint: `numpy.quantile()`)\n",
        "  - https://numpy.org/doc/stable/reference/generated/numpy.quantile.html\n",
        "- The **5th percentile** of the distribution\n",
        "- The **10th percentile** of the distribution\n",
        "- The **25th percentile** of the distribution\n",
        "- The **median** of the distribution (hint: `numpy.median()`)\n",
        "- The **mean** of the distribution (hint: `numpy.mean()`)\n",
        "- The **75th percentile** of the distribution\n",
        "- The **90th percentile** of the distribution\n",
        "- The **95th percentile** of the distribution\n",
        "- The **99th percentile** of the distribution\n",
        "- The **maximum** of the distribution (hint: `numpy.max()`)"
      ],
      "metadata": {
        "id": "_hmpwPKBV76e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary statistics\n",
        "print(\"Min:\", ...)\n",
        "print(\"1st Percentile:\", ...)\n",
        "print(\"5th Percentile:\", ...)\n",
        "print(\"10th Percentile:\", ...)\n",
        "print(\"25th Percentile:\", ...)\n",
        "print(\"Median:\", ...)\n",
        "print(\"Mean:\", ...)\n",
        "print(\"75th Percentile:\", ...)\n",
        "print(\"90th Percentile:\", ...)\n",
        "print(\"95th Percentile:\", ...)\n",
        "print(\"99th Percentile:\", ...)\n",
        "print(\"Max:\", ...)"
      ],
      "metadata": {
        "id": "kuA0TSyLAGt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4\n",
        "\n",
        "Summarize your findings below. Comment on the shape, center, and spread of the distribution of trending video counts, citing relevant summary statistics and interpreting the data in context."
      ],
      "metadata": {
        "id": "Yqe_fBQwWi_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(YOUR ANSWER GOES HERE.)"
      ],
      "metadata": {
        "id": "ZpE9o9y3WvZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6 - Trends and Engagement Over Time"
      ],
      "metadata": {
        "id": "ajk7qn-WW9D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1\n",
        "\n",
        "While not a perfect measure of platform activity, let's create a column in `trended_videos` called `\"Engagement\"` that equals the sum total of `views`,\t`likes`, `dislikes`, and `comment_count`."
      ],
      "metadata": {
        "id": "NmkMeAVTXrks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trended_videos[\"Engagement\"] = ...\n",
        "trended_videos.head()"
      ],
      "metadata": {
        "id": "c2fCASE9X5-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2\n",
        "\n",
        "Now, create a dataframe that calculates **total monthly engagement** for each `trending_date_year_month` date that appears in the dataframe. You will need to use `pandas.DataFrame.groupby()`. Save the grouped dataframe as `monthly_engagement` with a date column and a total engagement column.\n",
        "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
        "\n",
        "- **HINT**: you may need to use `pandas.DataFrame.reset_index()`\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"
      ],
      "metadata": {
        "id": "loLXi4p0ki60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_engagement = ...\n",
        "monthly_engagement"
      ],
      "metadata": {
        "id": "RWsvAxCImN3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3\n",
        "\n",
        "Now, using the `monthly_engagement` dataframe, create a time series line plot that plots total engagement for each month. You may either use:\n",
        "- `matplotlib.pyplot.plot()`\n",
        "  - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
        "\n",
        "or\n",
        "\n",
        "- `seaborn.lineplot()`\n",
        "  - https://seaborn.pydata.org/generated/seaborn.lineplot.html"
      ],
      "metadata": {
        "id": "skrU4iC8ml9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "oPc_PMmHm0Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Comment on your findings. What trend do you observe? Why might this be the case?"
      ],
      "metadata": {
        "id": "smzulpYUn6TX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(YOUR ANSWER GOES HERE)"
      ],
      "metadata": {
        "id": "qTroDrQzn9RL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5 Decomposing Engagement\n",
        "\n",
        "It can be helpful to know which components of engagement are actually driving the trend we observe. Redo the exercise from 4.3, but instead of aggregating just the `Engagement` column, aggregate the `views`, `likes`, `dislikes`, and `comment_count` columns and save the grouped and aggregated data into a dataframe called `monthly_components`.\n",
        "- **HINT**: you will need to use `pandas.DataFrame.reset_index()`\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"
      ],
      "metadata": {
        "id": "frwAnDOzoHBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_components = ...\n",
        "monthly_components"
      ],
      "metadata": {
        "id": "Dl6LAkGXok1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6 Four Different Lineplots\n",
        "\n",
        "Now, we are going to plot all four engagement variables on the same set of axes. Our first step is to generate a \"massaged\" version of our dataframe using `pandas.melt()`. This will create a dataframe that has each value associated with its respective engagement variable (likes, dislikes, comment count, or views) and the corresponding video.\n",
        "\n",
        "![pd.melt](https://pandas.pydata.org/docs/_images/reshaping_melt.png)\n",
        "\n",
        "Use the following documentation:\n",
        "- https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n",
        "- **HINT**: you will need to pass `\"trending_date_year_month\"` as the `id_vars` parameter in the function"
      ],
      "metadata": {
        "id": "DMswuBxbovZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_melted = ...\n",
        "monthly_melted"
      ],
      "metadata": {
        "id": "Kc82Q2Nxo9zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use either matplotlib or seaborn to generate a plot of 4 different time series for each engagement metric. Some helpful documentation below:\n",
        "- Matplotlib: https://www.geeksforgeeks.org/plot-multiple-lines-in-matplotlib/\n",
        "- Seaborn: https://stackoverflow.com/questions/52308749/how-do-i-create-a-multiline-plot-using-seaborn"
      ],
      "metadata": {
        "id": "OTHApskQrFxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "9Ju0rrdzrE6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7 If you worked as a data scientist at YouTube, which metric would you be trying to optimize to increase engagement and why?"
      ],
      "metadata": {
        "id": "3m0sWgGgrl0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(YOUR ANSWER GOES HERE.)"
      ],
      "metadata": {
        "id": "TqxLrr6Rrt_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7 - Correlated Metrics"
      ],
      "metadata": {
        "id": "9Z6VM4tHsRJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1\n",
        "\n",
        "Let's start by calculating the correlation coefficient between each pair of metrics: `views`, `likes`, `dislikes`, and `comment_count`.\n",
        "\n",
        "Rather than iterating over each one and manually calculating the correlation coefficient, we can make use of the function `pandas.DataFrame.corr()`.\n",
        "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
        "\n",
        "This will generate a **symmetric matrix** of the pair-wise correlation of each metric in our dataframe.\n",
        "\n",
        "As a first step, let's create a dataframe called `metrics` that only contains the columns `views`, `likes`, `dislikes`, and `comment_count` from `trended_videos`. Then, use the resulting dataframe and the `corr()` function above to generate a correlation matrix called `corr_matrix`."
      ],
      "metadata": {
        "id": "iNi1ulsysg0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ...\n",
        "metrics"
      ],
      "metadata": {
        "id": "RGit2kwttcAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = ...\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "9RvTovtZuRkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Correlation Heatmap\n",
        "\n",
        "Now, use the `seaborn` library to create a correlation **heatmap**. Documentation for the `seaborn.heatmap()` plotting function is below:\n",
        "- https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "**HINT**: if you want to display the correlation numbers on the heatmap, set the `annot` parameter accordingly.\n",
        "\n",
        "**BONUS**: One of Arnav's biggest pet peeves is poorly-colored correlation heatmaps, and it is his humble opinion that the default colors are terrible. If you feel the same way, Arnav would encourage you to play around with the color palette and make a more presentable heatmap. (You can change the `cmap` parameter.)"
      ],
      "metadata": {
        "id": "PeF7PeJxuZt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "lDPIzxLBuyq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3\n",
        "\n",
        "Interpret the correlation heatmap. Which variables are the most highly correlated? The least? Why do you think this is? If two variables have a correlation close to 1, what does that imply about their linear relationship?"
      ],
      "metadata": {
        "id": "bqtlWgHX89Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(YOUR ANSWER GOES HERE.)"
      ],
      "metadata": {
        "id": "Y35zQ_UR9TXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 - Scatterplots\n",
        "\n",
        "Create a scatterplot of the relationship between `likes` and `comment_count`. You may use either:\n",
        "- `matplotlib.pyplot.scatter()`\n",
        "  - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n",
        "\n",
        "or\n",
        "\n",
        "- `seaborn.scatterplot()`\n",
        "  - https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
        "\n",
        "**Make sure you add a title and axis labels!** To do that, you can use:\n",
        "- `matplotlib.pyplot.title`\n",
        "  - https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.title.html\n",
        "- `matplotlib.pyplot.xlabel` and `matplotlib.pyplot.ylabel`\n",
        "  - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html\n",
        "  - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html"
      ],
      "metadata": {
        "id": "BHewIQpq9Vsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "W2PP9RHS_kOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create a scatterplot with labeled axes and a title for the relationship between `likes` and `dislikes`."
      ],
      "metadata": {
        "id": "6COAviUEAVJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "xWFUtgevAcA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5\n",
        "\n",
        "Are these plots what you expected given their correlation? Explain your reasoning as to why the plots might look the way they do in spite of the correlation coefficient. What does this tell you about the correlation coefficient (what types of associations can it measure)?"
      ],
      "metadata": {
        "id": "2dchtHgwAlQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(YOUR ANSWER GOES HERE.)"
      ],
      "metadata": {
        "id": "fdwnSEtdAvKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8 - Disliked Videos (Topic Modeling)\n",
        "\n",
        "Let's investigate the most **disliked** YouTube videos that trended – videos that perhaps got popular for all the wrong reasons. Additionally, we'll make use of the two functions we gave you at the beginning of the lab (`get_topics` and `get_subtopics`) and do **topic modeling** to find out which \"themes\" or \"groups\" of topics were the most disliked. (This is a common task in natural language processing [NLP], but we unfortunately won't be able to go over it today...story for another day! Probably in DM.)"
      ],
      "metadata": {
        "id": "AI-FuNWjBgh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Selecting Specific Columns\n",
        "\n",
        "Let's create a new dataframe called `likes_and_dislikes` that only has the following columns from `trended_videos`:\n",
        "- `trending_date`\n",
        "- `title`\n",
        "- `channel_title`\n",
        "- `category_id`\n",
        "- `likes`\n",
        "- `dislikes`"
      ],
      "metadata": {
        "id": "BfGXh25cDD5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "likes_and_dislikes = ...\n",
        "likes_and_dislikes"
      ],
      "metadata": {
        "id": "u1nRhvUbDQ3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 String Column Concatenation\n",
        "\n",
        "Create a column in `likes_and_dislikes` called `\"title_and_channel\"`. This should be the result of concatenating the `\"title\"` and `\"channel_title\"` columns with a space (`\" \"`) in between. Your answer should make use of pandas/numpy vectorization abilities for Series/arrays containing strings!"
      ],
      "metadata": {
        "id": "g2NDleWNywuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "likes_and_dislikes[\"title_and_channel\"] = ...\n",
        "likes_and_dislikes"
      ],
      "metadata": {
        "id": "tOppdUlCzHZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Computing the Likes to Dislike Ratio\n",
        "\n",
        "Add a column called `like_dislike_ratio` to the `likes_and_dislikes` dataframe that contains entries for the like to dislike ratio for each video. Then, sort the dataframe by like to dislike ratio, starting from the most disliked videos to the least disliked videos (by ratio)."
      ],
      "metadata": {
        "id": "cUanhUBnQx0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "likes_and_dislikes[\"like_dislike_ratio\"] = ...\n",
        "likes_and_dislikes = ...\n",
        "likes_and_dislikes.head(30)"
      ],
      "metadata": {
        "id": "R8rMnUAwCNa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Most Disliked Videos\n",
        "\n",
        "Now, create a dataframe called `most_disliked` that only contains videos in `likes_and_dislikes` that have more dislikes than likes. (What can you filter `like_dislike_ratio` by to achieve this?)"
      ],
      "metadata": {
        "id": "CBMetm6pR1kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_disliked = ...\n",
        "most_disliked"
      ],
      "metadata": {
        "id": "CrlPBye2DSI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Most Disliked Topics with Web Scraping\n",
        "\n",
        "Now, we can finally make use of the topic modeling functions we defined earlier to see which \"themes\" or \"topics\" of videos get the most dislikes.\n",
        "\n",
        "Before we do that, though, YouTube actually has its own designated video category IDs. You can find them on the GitHub raw page below:\n",
        "- https://gist.githubusercontent.com/dgp/1b24bf2961521bd75d6c/raw/9abca29c436218972e6e28de7c03060845ed85d9/youtube%2520api%2520video%2520category%2520id%2520list\n",
        "\n",
        "Instead of manually hard-coding a list of these categories, let's get some practice automating this process using **web scraping**! For this task, we will use the `BeautifulSoup` library. We will import this library along with the `requests` module, which allows us to make HTTP requests to a specific website using Python."
      ],
      "metadata": {
        "id": "BPRG6HykScqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL - Import Web Scraping Tools\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "h1b56QOXZXJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, fill in the `url` variable with the link to the webpage we are trying to scrape. After you do this, notice that:\n",
        "\n",
        "1. We first make a request to the specified URL using `requests.get(url)`. In more technical terms, what this does is retrieve data from a server in the form of the webpage. When a URL is entered into a web browser, the browser sends a GET request to the server hosting the website, and the server responds by sending the requested data back.\n",
        "2. We then use `BeautifulSoup` to get the `.text` attribute from the requested webpage using `html.parser`. This is essentially what does the scraping task for us and loads in all the text from the requested webpage."
      ],
      "metadata": {
        "id": "6DvxZypXZoPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = ... # YOUR CODE GOES HERE (copy the GitHub URL here)\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "metadata": {
        "id": "LgbmO3AtV-pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use the `find_all()` function from `BeautifulSoup` to generate all elements on the webpage for us."
      ],
      "metadata": {
        "id": "6Na7F3jBaXWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "texts = soup.find_all(string=True)\n",
        "texts"
      ],
      "metadata": {
        "id": "e1RVGURZXNw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINALLY, YOUR TASK:**\n",
        "\n",
        "The `texts` variable is a one-element list containing all the string text from the category IDs GitHub raw webpage.\n",
        "\n",
        "Each category ID in the list is separated by the newline separator, `'\\n'`. We are also interested in only the first 32 category IDs presented in the list. (The rest are redundant.)\n",
        "\n",
        "Your task is to use Python string parsing functions to create a pandas dataframe called `df_categories` that contains two columns:\n",
        "1. `category_id`, an **integer** column of all category IDs in the list\n",
        "2. `category_name`, which is the corresponding name for each category ID in the list.\n",
        "\n",
        "This is an open-ended question, but it can also be challenging! Here are some Python tricks and functions our solution used:\n",
        "\n",
        "- used the `string.split()` function twice\n",
        "- used list slicing to get the first 32 category IDs\n",
        "- created two lists/arrays to store each category ID and name and iterated through all of the given categories\n",
        "- used `np.array([]).astype(int)` to convert the list of category IDs into an integer array rather than a string array\n",
        "- created a new pandas Dataframe with two columns called `category_id` and `category_name`"
      ],
      "metadata": {
        "id": "g0tTfy0QahhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "\n",
        "df_categories = pd.DataFrame({...})\n",
        "df_categories"
      ],
      "metadata": {
        "id": "nEneMrxPYGdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, merge the `df_categories` dataframe categories with the categories in `most_disliked`. No rows should get ddropped from `most_disliked`. Each one should have a corresponding category ID from `df_categories`.\n",
        "\n",
        "You can call the resulting dataframe `most_disliked_categories`.\n",
        "\n",
        "A reminder on how the pandas merge syntax works:\n",
        "- https://pandas.pydata.org/docs/reference/api/pandas.merge.html#pandas.merge"
      ],
      "metadata": {
        "id": "PqlM6y-Zb7vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_disliked_categories = ...\n",
        "most_disliked_categories"
      ],
      "metadata": {
        "id": "8fp0et5TcOQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's generate two **bar plots**:\n",
        "\n",
        "1. A bar plot of the most disliked `category_name`s by count\n",
        "2. A bar plot of each `category_name`'s average like to dislike ratio\n",
        "\n",
        "You know the drill by now: use grouping and aggregation to create the intermediate dataframes, then use a plotting library to generate bar plots. You may either use:\n",
        "\n",
        "- `matplotlib.pyplot.bar()`\n",
        "  - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html\n",
        "\n",
        "or\n",
        "\n",
        "- `seaborn.barplot()`\n",
        "  - https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
        "\n",
        "\n",
        "If your x-axis labels are overlapping, I would recommend adding the following code above your bar plot:\n",
        "\n",
        "`plt.figure(figsize=(20, 10))`"
      ],
      "metadata": {
        "id": "ap_aI-JtdGcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_disliked_counts = ...\n",
        "most_disliked_counts"
      ],
      "metadata": {
        "id": "3rDcQPJGduMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "6_-pcyQRFJAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_disliked_avg = ...\n",
        "most_disliked_avg"
      ],
      "metadata": {
        "id": "KXpjyqVHFhr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "lZFXc1MjfAdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Generating Subtopics with a Language Model (NLP)\n",
        "\n",
        "If you look at these official YouTube categories, they may seem a little unspecific. For example, \"News & Politics\" gets the most dislikes, and \"People & Blogs\" has the lowest like to dislike ratio. However, \"People & Blogs\" also includes political videos like Roy Moore's concession speech. It would be better if we had a way to get more specific topics based on our data!\n",
        "\n",
        "Let's use our `get_subtopics()` function defined at the beginning of the lab. This function makes use of the small language model DeBERTa to assign subtopic labels to each video in our dataset based on the title of the YouTube video and the channel name (`title_and_channel`).\n",
        "\n",
        "As a reminder, the function syntax is below:"
      ],
      "metadata": {
        "id": "lXWUlv5zfpY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST RUN THIS CELL\n",
        "get_subtopics(\"Insane Three-Point Shot Made by Stephen Curry! | Lakers vs. Warriors Basketball Highlights NBA\", subtopic_labels)"
      ],
      "metadata": {
        "id": "hghPCz3VyYrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, for the task.**\n",
        "\n",
        "Using `pandas.Series.apply()`, call the `get_subtopics()` function on the `title_and_channel` column in the `most_disliked_categories` dataframe. Save the resulting Series as a new column in `most_disliked_categories` called `subtopic`.\n",
        "- **Hint**: Documentation for `pandas.Series.apply()` is below. Rather than pass in the function name directly, you should pass in a `lambda` function that calls `subtopic_classifier` and pass in the `subtopic_labels` you generated. Syntax examples below:\n",
        "\n",
        "  - https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html\n",
        "  - https://stackoverflow.com/questions/19914937/applying-function-with-multiple-arguments-to-create-a-new-pandas-column\n",
        "\n",
        "- **NOTE**: This may take around 4 to 5 minutes to run. Don't worry! That's normal for more complicated ML tasks. Go take a stretch break while it runs :)"
      ],
      "metadata": {
        "id": "9SWMncf5qz2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_disliked_categories[\"subtopic\"] = ...\n",
        "most_disliked_categories"
      ],
      "metadata": {
        "id": "lTI-7WvvfFFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.7 Do you agree with the subtopic labels? Which ones seem correct and which ones seem off? Feel free to randomly sample from the dataframe to make your judgment."
      ],
      "metadata": {
        "id": "5Enw--n53lX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(YOUR ANSWER GOES HERE.)"
      ],
      "metadata": {
        "id": "qs-CY9Go3tEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.8 Subtopic Bar Plots\n",
        "\n",
        "Just like you did for the category names, generate two bar plots for the subtopics:\n",
        "1. One bar plot getting the count of each subtopic in the `most_disliked_categories` dataframe\n",
        "2. One bar plot getting the average like to dislike ratio of each subtopic in the `most_disliked_categories` dataframe.\n",
        "\n",
        "Feel free to copy-paste and modify your code from above for this question!"
      ],
      "metadata": {
        "id": "VAn4Nka81FCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_disliked_subtopics_counts = ...\n",
        "most_disliked_subtopics_counts"
      ],
      "metadata": {
        "id": "8pwA6SfLyoUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "2Q3PmR4r1jTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_disliked_subtopics_avg = ...\n",
        "most_disliked_subtopics_avg"
      ],
      "metadata": {
        "id": "pKfZQczy1p2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "ZUPi1FPE1yqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulations! You have finished lab 3 🎉\n",
        "\n",
        "Make sure you submit your lab notebook to Gradescope!"
      ],
      "metadata": {
        "id": "B93UHM5w1_lI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6cfshh912U0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}